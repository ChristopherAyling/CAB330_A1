{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAB330 Case Study 1: Students\n",
    "\n",
    "Class: CAB330\n",
    "\n",
    "Students: \n",
    "\n",
    "- Christopher Ayling | christopher.ayling@connect.qut.edu.au\n",
    "- Benjamin Saljooghi | 9448233 | benjamin.saljooghi@connect.qut.edu.au\n",
    "- Jordi Smit | 10294139 | jordi.smit@connect.qut.edu.au\n",
    "\n",
    "Due Date: 9th September 2018\n",
    "\n",
    "Project Demo: Week 8 Wednesday Lab\n",
    "\n",
    "Weighting: 25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulating Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "\n",
    "# Visualisations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG, Image\n",
    "import graphviz\n",
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSeed = 330\n",
    "np.random.seed(randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.read_csv(\"./STUDENT.csv\")\n",
    "students.head()\n",
    "\n",
    "rows, columns = students.shape; rows, columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1. Data Selection and Distribution. (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Descriptions\n",
    "\n",
    "The following information would assist you in assigning the variables roles.\n",
    "\n",
    "- There are three target variables namely, G1, G2 and G3, with different types. Choose the target that suits best according to the given task.\n",
    "- Identify if the variable is an input variable or a supplementary variable.\n",
    "- Data transformation is required for a few input variables to get improved accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Proportion of Students Who Will Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G3_counts = students['G3'].value_counts()\n",
    "plt.pie(G3_counts, labels=G3_counts.index, startangle=90, shadow=True, explode=(0, 0.1), autopct='%1.1f%%')\n",
    "plt.title(\"Proportion of Students Who Will Pass\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = students.copy()\n",
    "\n",
    "# impute age NAs with mean\n",
    "cleaned['age'].fillna(cleaned['age'].mean(), inplace=True)\n",
    "\n",
    "# impute reason NAs with unknown value because else it will be read as float instead of a string, which cause trouble with the hot one transformation.\n",
    "cleaned['reason'].fillna(\"-\", inplace=True)\n",
    "\n",
    "# impute school NAs with unknown value because else it will be read as float instead of a string, which cause trouble with the hot one transformation..\n",
    "cleaned['school'].fillna(\"-\", inplace=True)\n",
    "\n",
    "# drop unused columns\n",
    "cleaned.drop(columns=[\"id\", \"InitialName\"], inplace=True)\n",
    "\n",
    "#TODO maby better to remove missing target variable?\n",
    "# impute G1 NAs with mean\n",
    "cleaned['G1'].fillna(cleaned['G1'].mean(), inplace=True)\n",
    "\n",
    "# impute G2 NAs with mean\n",
    "cleaned['G2'].fillna(cleaned['G2'].mean(), inplace=True)\n",
    "\n",
    "#TODO Should we also check for outliers or inconsistencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Level of Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = {\n",
    "    \"id\": [\"student's id\", False, False, 'nominal'],\n",
    "    \"InitialName\": [\"student's initial\", False, False, 'nominal'],\n",
    "    \"school\": [\"student's school name\", True, True, 'nominal'],\n",
    "    \"sex\": [\"student's sex\", True, True, 'nominal'],\n",
    "    \"age\": [\"student's age\", True, True, 'numerical'],\n",
    "    \"address\": [\"student's home address type\", True, True, 'nominal'],\n",
    "    \"famsize\": [\"family size (≤ 3 or > 3)\", True, True, 'ordinal'],\n",
    "    \"Pstatus\": [\"parent's cohabitation status (living together or apart)\", True, True, 'nominal'],\n",
    "    \"Medu\": [\"mother’s education(0 – none, 1 – primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\", True, True, 'ordinal'],\n",
    "    \"Fedu\": [\"father’s education(0 – none, 1 – primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\", True, True, 'ordinal'],\n",
    "    \"Mjob\": [\"mother's job\", True, True, 'nominal'],\n",
    "    \"Fjob\": [\"father's job\", True, True, 'nominal'],\n",
    "    \"reason\": [\"reason to choose this school\", True, True, 'nominal'],\n",
    "    \"guardian\": [\"student's guardian\", True, True, 'nominal'],\n",
    "    \"traveltime\": [\"home to school travel time (1 – < 15 min., 2 – 15 to 30 min., 3 – 30 min. to 1 hour or 4 – > 1 hour)\", True, True, 'ordinal'],\n",
    "    \"studytime\": [\"weekly study time (1 – < 2 hours, 2 – 2 to 5 hours, 3 – 5 to 10 hours or 4 – > 10 hours)\", True, True, 'ordinal'],\n",
    "    \"failures\": [\"number of past class failures(n if 1 ≤ n < 3, else 4)\", True, True, 'ordinal'],\n",
    "    \"schoolsup\": [\"extra educational school support (yes or no)\", True, True, 'nominal'],\n",
    "    \"famsup\": [\"family educational support (yes or no)\", True, True, 'nominal'],\n",
    "    \"paid\": [\"extra paid classes (yes or no)\", True, True, 'nominal'],\n",
    "    \"activities\": [\"extra-curricular activities (yes or no)\", True, True, 'nominal'],\n",
    "    \"nursery\": [\"attended nursery school (yes or no)\", True, True, 'nominal'],\n",
    "    \"higher\": [\"wants to take higher education (yes or no)\", True, True, 'nominal'],\n",
    "    \"internet\": [\"Internet access at home (yes or no)\", True, True, 'nominal'],\n",
    "    \"romantic\": [\"with a romantic relationship (yes or no)\", True, True, 'nominal'],\n",
    "    \"famrel\": [\"quality of family relationships (1 – very bad to 5 – excellent)\", True, True, 'ordinal'],\n",
    "    \"freetime\": [\"free time after school (1 – very low to 5 – very high)\", True, True, 'ordinal'],\n",
    "    \"goout\": [\"going out with friends (1 – very low to 5 – very high)\", True, True, 'ordinal'],\n",
    "    \"Dalc\": [\"workday alcohol consumption (1 – very low to 5 – very high)\", True, True, 'ordinal'],\n",
    "    \"Walc\": [\"weekend alcohol consumption (1 – very low to 5 – very high)\", True, True, 'ordinal'],\n",
    "    \"health\": [\"current health status (1 – very bad to 5 – very good)\", True, True, 'ordinal'],\n",
    "    \"absences\": [\"number of school absences (0 to 75)\", True, True, 'numerical'],\n",
    "    \"G1\": [\"first period grade (0 to 20)\", True, True, 'numerical', True],\n",
    "    \"G2\": [\"second period grade (0 to 20)\", True, True, 'numerical', True],\n",
    "    \"G3\": [\"Final result (PASS/FAIL)\", True, True, 'nominal', True],\n",
    "}\n",
    "\n",
    "red = 'background-color: Tomato'\n",
    "green = 'background-color: MediumSeaGreen'\n",
    "blue = 'background-color: DodgerBlue'\n",
    "sblue = 'background-color: SlateBlue'\n",
    "violet = 'background-color: Violet'\n",
    "gray = 'background-color: Lightgray'\n",
    "orange = 'background-color: Orange'\n",
    "\n",
    "def highlight_useful(val):\n",
    "    \"\"\" highlight True as green, false as red \"\"\"\n",
    "    return '' if val is None else green if val else red\n",
    "\n",
    "def highlight_type(val):\n",
    "    \"\"\" highlight based on variable type \"\"\"\n",
    "    if val == 'nominal':\n",
    "        return blue\n",
    "    elif val == 'ordinal':\n",
    "        return sblue\n",
    "    elif  val == 'numerical':\n",
    "        return violet\n",
    "    \n",
    "def highlight_target(val):\n",
    "    \"\"\" highlight if target variable \"\"\"\n",
    "    return orange if val else gray\n",
    "\n",
    "\n",
    "COLUMN_INFORMATION = pd.DataFrame.from_dict(descriptions, orient='index')\n",
    "COLUMN_INFORMATION.columns = ['Description', 'For Classification', 'For Regression', 'Variable Type', 'Target']\n",
    "COLUMN_INFORMATION = COLUMN_INFORMATION[['Description', 'Target', 'Variable Type', 'For Classification', 'For Regression']]\n",
    "COLUMN_INFORMATION['Target'] = COLUMN_INFORMATION['Target'].apply(lambda cell: bool(cell))\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', -1):\n",
    "    display(\n",
    "        COLUMN_INFORMATION.style.\\\n",
    "            applymap(highlight_useful, subset=['For Classification', 'For Regression']).\\\n",
    "            applymap(highlight_type, subset='Variable Type').\\\n",
    "            applymap(highlight_target, subset='Target')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Distribution Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variables = ['G3']\n",
    "features = cleaned.loc[:, cleaned.columns.difference(target_variables)]\n",
    "targets = cleaned.loc[:, target_variables]\n",
    "\n",
    "test_size = 0.2\n",
    "training_size = 1.0 - test_size\n",
    "X_training, X_test, Y_training, Y_test = train_test_split(features, targets, test_size=test_size, train_size=training_size, random_state=randomSeed, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_overview = {\n",
    "    \"decision_tree\": {},\n",
    "    \"regression\": {},\n",
    "    \"neural_network\": {},\n",
    "}\n",
    "\n",
    "def format_accuracy_overview(Y, predicted_Y, most_important_features):\n",
    "    return {\n",
    "        \"precision\": precision_score(Y, predicted_Y),\n",
    "        \"recall\": recall_score(Y, predicted_Y),\n",
    "        \"f1\": f1_score(Y, predicted_Y),\n",
    "        \"accuracy\": accuracy_score(Y, predicted_Y),\n",
    "        \"most_important_features\": most_important_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2. Predictive Modeling Using Decision Trees\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Build a decision tree using default setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a lable encoder for each of the nominal and ordinal features.\n",
    "DT_label_encoders = {}\n",
    "for name in descriptions:\n",
    "    if (\"nominal\" in descriptions[name] or \"ordinal\" in descriptions[name]) and name in cleaned.columns.values.tolist():\n",
    "        lb = preprocessing.LabelEncoder()\n",
    "        #Check cleanded data for every possible class. If only done on training data it might miss some.\n",
    "        lb.fit(cleaned[name].tolist())\n",
    "        DT_label_encoders[name] = lb \n",
    "\n",
    "def transform_features(Data, encoders):\n",
    "    \"\"\"Transforms data based on the provided encoder\"\"\"\n",
    "    Data_copy = Data.copy()\n",
    "    for col_name in Data_copy.columns.values.tolist():\n",
    "        if col_name in encoders:\n",
    "            #Get encoder\n",
    "            encoder = encoders[col_name]\n",
    "            #Transform the data in this col\n",
    "            col_values = Data_copy[col_name].tolist()\n",
    "            Data_copy[col_name] = encoder.transform(col_values)\n",
    "            \n",
    "    return Data_copy\n",
    "\n",
    "def transform_features_to_DT(Data):\n",
    "    \"\"\"Transforms the nominal and ordinal features into a format that can be compared by the decision tree\"\"\"\n",
    "    return transform_features(Data, DT_label_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the training data into a format with which the decision tree can work.\n",
    "X_training_decision_tree_format = transform_features_to_DT(X_training) \n",
    "Y_training_decision_tree_format = transform_features_to_DT(Y_training) \n",
    "\n",
    "\n",
    "#Create a decision tree and train it on the formated training data.\n",
    "dt = DecisionTreeClassifier(random_state=randomSeed)\n",
    "dt.fit(X_training_decision_tree_format, Y_training_decision_tree_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. What is the classification accuracy on training and test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform test data\n",
    "X_test_decision_tree_format = transform_features_to_DT(X_test) \n",
    "Y_test_decision_tree_format = transform_features_to_DT(Y_test) \n",
    "\n",
    "#Create a dataframe to display the values.\n",
    "training_score = dt.score(X_training_decision_tree_format, Y_training_decision_tree_format)\n",
    "test_score = dt.score(X_test_decision_tree_format, Y_test_decision_tree_format)    \n",
    "dt_preformance = pd.DataFrame([training_score, test_score], columns=target_variables, index=['Train accuracy', 'Test accuracy'])\n",
    "dt_preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. List the decision rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_features = X_training_decision_tree_format.columns.values.tolist()\n",
    "dotfile = StringIO()\n",
    "export_graphviz(dt, out_file=dotfile, feature_names=dt_features)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"default_tree.png\") # saved in the following file - will return True if successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique decision rules:\n",
    " - G2 <= 10.45\n",
    " - G2 <-=9.55\n",
    " - G1 <= 9.75\n",
    " - goout <=0.5\n",
    " - failures <= 0.5\n",
    " - sex <= 0.5\n",
    " - G2 <= 9.35\n",
    " - Medu <= 9.35\n",
    " - goout <= 3.5\n",
    " - Dalc < 3.5\n",
    " - G1 <= 9.85\n",
    " - age <= 17.5\n",
    " - Fedu <= 1.5\n",
    " - Fjob <= 0.5\n",
    " - studytime <= 1.0\n",
    " - age <= 16.868\n",
    " - guardian <= 1.5\n",
    " - absences <= 6.0\n",
    " - absences <= 9.0\n",
    " - G1 <= 11.225\n",
    " - famrel <= 2.5\n",
    " - walc <= 0.5\n",
    " - guardian <= 0.5\n",
    " - walc <= 2.5\n",
    " - dalc <= 0.5\n",
    " - Mjob <= 1.5\n",
    " - Medu <= 1.5\n",
    " - adress <= 0.5\n",
    " - Fedu <= 2.5\n",
    " - health <= 3.0\n",
    " - school <= 1.5\n",
    " - reason <= 2.5\n",
    " - freetime <= 2.5\n",
    " - G2 <= 11.35\n",
    " - G1 <= 11.55\n",
    " - traveltime <= 0.5\n",
    " - Fjob <= 1.5\n",
    " - reason <= 0.5\n",
    " - Walc <= 3.5\n",
    " - Walc <= 0.5\n",
    " - health <= 0.5\n",
    " - G1 <= 10.85\n",
    " - reason <= 1.5\n",
    " - paid <= 0.5\n",
    " - activities <= 0.5\n",
    " - nursery <= 0.5\n",
    " - G1 <= 10.55\n",
    " - famrel <= 3.5\n",
    " - G2 <= 11.55\n",
    " - G1 <= 12.85\n",
    " - schoolsup <= 0.5\n",
    " - absences <= 1.0\n",
    " - goout <= 05\n",
    " - Walc < 3.5\n",
    "\n",
    "The complete decision tree:\n",
    "\n",
    "![](default_tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. What are the 5 important variables in building the tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = dt.feature_importances_\n",
    "importances_dt = pd.DataFrame(importances, index=dt_features, columns=[\"G3\"])\n",
    "importances_dt.nlargest(5, \"G3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Report if you see any evidence of model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is model overfitting on the training set since the decision tree has a 100% accuracy at the training set while it only has a 82.8% accuracy on the test set. This can also be seen in the tree since it has become extremely complicated with very many paths. This a classic example of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build another decision tree tuned with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The parameters to be searched\n",
    "parameters = {\n",
    "     \"max_depth\": [None, 1, 2, 3, 4, 5],\n",
    "     \"min_samples_split\": [0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "     \"min_samples_leaf\": [1, 2, 4, 8, 16, 32],\n",
    "     \"criterion\": ['gini', 'entropy'],\n",
    "     \"splitter\" : [\"best\", \"random\"],\n",
    "     \"max_features\": [None, \"auto\", \"sqrt\", \"log2\"],\n",
    "     \"max_leaf_nodes\": [None, 2, 3, 4, 5, 6],\n",
    "}\n",
    "#creates and starts a grids search\n",
    "gs_dt = GridSearchCV(DecisionTreeClassifier(random_state=randomSeed), parameters, n_jobs=8)\n",
    "gs_dt.fit(X_training_decision_tree_format, Y_training_decision_tree_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. What is the classification accuracy on training and test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc scores\n",
    "training_score = gs_dt.score(X_training_decision_tree_format, Y_training_decision_tree_format)\n",
    "test_score = gs_dt.score(X_test_decision_tree_format, Y_test_decision_tree_format)\n",
    "\n",
    "#Display training and test score\n",
    "dt_grid_preformance = pd.DataFrame([training_score, test_score], columns=[\"G3 grid\"], index=['Train accuracy', 'Test accuracy'])\n",
    "dt_grid_preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_y_predicted = gs_dt.best_estimator_.predict(X_test_decision_tree_format)\n",
    "dt_score_overview = classification_report(Y_test_decision_tree_format, dt_y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What are the parameters used? Explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are being considered: \n",
    " - **max_depth**: Limits the maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    " - **min_samples_split**: The minimum number of samples required to split an internal node.\n",
    " - **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n",
    " - **criterion**: The function to measure the quality of a split. Examples are Gini impurity and entropy/information gain.\n",
    " - **splitter**: The strategy used to choose the split at each node. Examples are best which choices the best criterion criteria or random which preforms a random split.\n",
    " - **max_features**:  Limits the number of features to consider when looking for the best split. For Example sqrt limits it to sqrt (total_number_of_features).\n",
    " - **max_leaf_nodes**:  Limits the number of leaf nodes. Nodes with relative reduction in impurity are added first to ensure the best possible tree with the constaint.\n",
    " \n",
    "We decided to use the kitchen sink approach. We looked up the default values of the parameters and provided the grid search with a random range of values arround these default values. Then we the GridSearch run on multiple threads/jobs and we see which parameters return the optimal values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. What are the optimal parameters for this decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The following parameters result in the best decision tree:\")\n",
    "for name in parameters:\n",
    "    print(f\"   -{name}: {gs_dt.best_params_[name]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Which variable is used for the first split? What are the competing splits for this first split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_features = X_training_decision_tree_format.columns.values.tolist()\n",
    "dotfile = StringIO()\n",
    "export_graphviz(gs_dt.best_estimator_, out_file=dotfile, feature_names=dt_features)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"grid_tree.png\") # saved in the following file - will return True if successful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first split is done based on the following rule: G2 <= 10.45. The split rules at the second level are G2 <= 9.55 & G2 <= 11.35.\n",
    "Which is the same as the tree with the default parameter. The only different is that this tree has far less leaf nodes.\n",
    "\n",
    "The result:\n",
    "\n",
    "![](grid_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. What are the 5 important variables in building the tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the importances of each feature\n",
    "importances = gs_dt.best_estimator_.feature_importances_\n",
    "\n",
    "#display the 5 most important features\n",
    "importances_dt_gs = pd.DataFrame(importances, index=dt_features, columns=[\"G3\"])\n",
    "importances_dt_gs\n",
    "importances_dt_gs.nlargest(5, \"G3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_5_most_imporant_features = importances_dt_gs.nlargest(5, \"G3\").index.values.tolist()\n",
    "accuracy_overview[\"decision_tree\"] = format_accuracy_overview(Y_test_decision_tree_format, dt_y_predicted, dt_5_most_imporant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f. Report if you see an evidence of model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no evidence of model overfitting, the accuracy on the train and test sets moved much closer to each other. The tree has also become much simpler compared to the original one as can be seen in the visualization image. This is also a good indicater that there is no overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 What is the significant difference do you see between these two decision tree models? How do they compare performance-wise? Explain why those changes may have happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the significant difference do you see between these two decision tree models?**\n",
    "When you compare the visualization of the trees, the first thing you notice is that the grid tree is much smaller and simpler than the original one. \n",
    "The grid tree also does not have duplicated rules. The original thee had some duplicated rules in different branches. The original tree did this to fit the training data perfectly which resulted into over-fitting.\n",
    "\n",
    "**How do they compare performance-wise**\n",
    "The original tree fitted the training data perfectly  but only achieved an 82% accuracy on the test data due to over-fitting. While the grid tree only achieve a 93% accuracy on the training data, it achieved a 86% accuracy on the test data. Thus the grid tree is much better at generalizing, which result into a higher accuracy on data it has never seen before. Making the grid tree the better one.\n",
    "\n",
    "**Explain why those changes may have happened**\n",
    "These changes happen due to the grid search. Sklearning's grid search uses K-fold validation on each possible configuration. The K-fold validation prevents\n",
    " the training algorithm from over-fitting in combination with the configuration parameters. Eventually the search returns the configuration that has the highest K-fold score which result into a tree data is not over-fitted (assuming that the training data is representative).\n",
    "In our case this resulted into a much smaller tree. A much smaller tree means that there are fewer rules and that the decision rules are much more general and have to focus on the most important features. This means that the tree cannot perfectly fit the training data and over-fit, which makes it preform better on data it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparision = pd.DataFrame([], columns=[])\n",
    "print(comparision.append(dt_preformance[\"G3\"]).append(dt_grid_preformance[\"G3 grid\"]).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 From the better model, can you identify which students to target for further consultation? Can you provide some descriptive summary of those students?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following rules identify students that will most likely fail.\n",
    "\n",
    "- `if G2 <= 9.55` There are 206 students in this group and 95.6% of them eventually failed. So this is the most important group to watch.\n",
    "- `if 9.55 < G2 <= 10.45 & G1 <= 11.225` There are 73 students in this group and 72.6% of them eventually failed. So this is the second most important group to watch.\n",
    "\n",
    "The following rules are far less interesting but still indicate students with a small changes of failing. \n",
    "- `if 10.45 < G2 <= 11.35 & G1 <= 11.55` There are 77 students in this group and 27.3% of them eventually failed.\n",
    "- `if 9.55 < G2 <= 10.45 & G1 > 11.225` There are 16 students in this group and 18.8% of them eventually failed. \n",
    "\n",
    "As can be seen in the rules above only the grades provide a good indication about wheter the students will pass or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 3. Predictive Modeling Using Regression\n",
    "\n",
    "(5.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Apply transformation/scaling methods to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(data, column):\n",
    "    dummies = pd.get_dummies(data[column], prefix=column)\n",
    "    data = data.drop(column, axis=1)\n",
    "    data = data.join(dummies)\n",
    "    return data\n",
    "\n",
    "def binary(data, column, map):\n",
    "    new_col = data[column].map(map)\n",
    "    data = data.drop(column, axis=1)\n",
    "    data = data.join(new_col)\n",
    "    return data\n",
    "    \n",
    "def transform(data):\n",
    "    one_hot_cols = ['school', 'address', 'Mjob', 'Fjob', 'reason', 'guardian']\n",
    "    yes_no_cols = ['schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']\n",
    "    yes_no_map = {'no':0, 'yes':1}\n",
    "    \n",
    "    for col in one_hot_cols:\n",
    "        data = one_hot(data, col)\n",
    "    \n",
    "    for col in yes_no_cols:\n",
    "        data = binary(data, col, yes_no_map)\n",
    "    \n",
    "    data = binary(data, 'sex', {'F':0, 'M':1})\n",
    "    data = binary(data, 'famsize', {'LE3':0, 'GT3':1})\n",
    "    data = binary(data, 'Pstatus', {'A':0, 'T':1})\n",
    "    data = binary(data, 'G3', {'FAIL':0, 'PASS':1})    \n",
    "    return data\n",
    "\n",
    "# get a copy of the data set and transform it\n",
    "reg_data = transform(cleaned.copy())\n",
    "\n",
    "# visualize transformed data\n",
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', None):\n",
    "    display(reg_data)\n",
    "\n",
    "# split transformed data into features and targets\n",
    "reg_features = reg_data.loc[:, reg_data.columns.difference(target_variables)]\n",
    "reg_targets = reg_data.loc[:, target_variables]\n",
    "\n",
    "# train test split the transformed data\n",
    "reg_X_train_frame, reg_X_test_frame, reg_Y_train_frame, reg_Y_test_frame = train_test_split(reg_features, reg_targets, test_size=test_size, train_size=training_size, random_state=randomSeed, shuffle=False)\n",
    "    \n",
    "# convert to numpy matrices\n",
    "reg_X_train = reg_X_train_frame.values\n",
    "reg_X_test = reg_X_test_frame.values\n",
    "reg_Y_train = reg_Y_train_frame.values\n",
    "reg_Y_test = reg_Y_test_frame.values\n",
    "# make Y arrays one-dimensional\n",
    "reg_Y_train =  np.ravel(reg_Y_train)\n",
    "reg_Y_test = np.ravel(reg_Y_test)\n",
    "\n",
    "# apply scaling\n",
    "scaler = StandardScaler()\n",
    "# learn the mean and std.dev of variables from training data\n",
    "# then use the learned values to transform training data\n",
    "reg_X_train = scaler.fit_transform(reg_X_train, reg_Y_train)    \n",
    "# transform the test data with the scaler that was trained on the training data\n",
    "reg_X_test = scaler.transform(reg_X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build regression models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"reg\": None,\n",
    "    \"reg_cv\": None,\n",
    "    \"reg_rfe\": None,\n",
    "    \"reg_cv_rfe\": None,\n",
    "}\n",
    "\n",
    "# Evaluation\n",
    "def report_overfitting(model_name, X_train, X_test):\n",
    "    print(\"Train accuracy:\", models[model_name].score(X_train, reg_Y_train))\n",
    "    print(\"Test accuracy:\", models[model_name].score(X_test, reg_Y_test))\n",
    "    \n",
    "def report_accuracy(model_name, X_test):\n",
    "    print(classification_report(reg_Y_test, models[model_name].predict(X_test)))\n",
    "    \n",
    "models[\"reg\"] = LogisticRegression()\n",
    "models[\"reg\"].fit(reg_X_train, reg_Y_train)\n",
    "\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "models[\"reg_cv\"] = GridSearchCV(LogisticRegression(), params, cv=10, n_jobs=-1)\n",
    "models[\"reg_cv\"].fit(reg_X_train, reg_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Report which variables are included in the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = reg_X_train_frame.columns\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Report the top-5 important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = models[\"reg\"].coef_[0]\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "top_5 = [(feature_names[i], coef[i]) for i in indices[:5]]\n",
    "for feature, coefficient in top_5:\n",
    "    print(feature, coefficient) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Report any sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without GridSearchCV\")\n",
    "report_overfitting(\"reg\", reg_X_train, reg_X_test)\n",
    "\n",
    "print(\"\\nWith GridSearchCV\")\n",
    "report_overfitting(\"reg_cv\", reg_X_train, reg_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. What are the parameters used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the parameters?\n",
    "print(\"Parameters:\")\n",
    "print(models[\"reg\"].get_params(), \"\\n\")\n",
    "    \n",
    "# Explain your decision.\n",
    "print(\"These parameters are used because they are the defaults.\\n\")\n",
    "\n",
    "# What are the optimal parameters?\n",
    "print(\"Optimal parameters:\")\n",
    "print(models[\"reg_cv\"].best_params_, \"\\n\")\n",
    "\n",
    "# Which regression function is being used?\n",
    "print(\"Logistic regression is used because we have a binary classification problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. What is the classification accuracy on training and test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without GridSearchCV\")\n",
    "report_accuracy(\"reg\", reg_X_test)\n",
    "\n",
    "print(\"\\nWith GridSearchCV\")\n",
    "report_accuracy(\"reg_cv\", reg_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build another regression model using the subset of inputs selected by RFE and selection by model methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFECV(estimator = LogisticRegression(), cv=10)\n",
    "rfe.fit(reg_X_train, reg_Y_train)\n",
    "reg_X_train_sel = rfe.transform(reg_X_train)\n",
    "reg_X_test_sel = rfe.transform(reg_X_test)\n",
    "\n",
    "models[\"reg_rfe\"] = LogisticRegression()\n",
    "models[\"reg_rfe\"].fit(reg_X_train_sel, reg_Y_train)\n",
    "\n",
    "# hyperparameters = dict(C=np.logspace(0, 4, 10), penalty=['l1', 'l2'])\n",
    "# models[\"reg_cv_rfe\"] = GridSearchCV(LogisticRegression(), hyperparameters, cv=10, n_jobs=-1)\n",
    "# models[\"reg_cv_rfe\"].fit(reg_X_train_sel, reg_Y_train)\n",
    "# models[\"reg_cv_rfe\"] = LogisticRegressionCV()\n",
    "# models[\"reg_cv_rfe\"].fit(reg_X_train_sel, reg_Y_train)\n",
    "\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "models[\"reg_cv_rfe\"] = GridSearchCV(LogisticRegression(), params, cv=10, n_jobs=-1)\n",
    "models[\"reg_cv_rfe\"].fit(reg_X_train_sel, reg_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  a. Report which variables are included in the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for boolean, feature in zip(rfe.support_, feature_names):\n",
    "    if boolean:\n",
    "        print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Report the top-5 important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_sorted_features = []\n",
    "for i in range(len(rfe.ranking_)):\n",
    "    for index in range(len(rfe.ranking_)):\n",
    "        rank = rfe.ranking_[index]\n",
    "        if (rank == i+1):\n",
    "            rfe_sorted_features.append(feature_names[index])\n",
    "print(rfe_sorted_features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Report any sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without GridSearchCV\")\n",
    "report_overfitting(\"reg_rfe\", reg_X_train_sel, reg_X_test_sel)\n",
    "\n",
    "print(\"\\nWith GridSearchCV\")\n",
    "report_overfitting(\"reg_cv_rfe\", reg_X_train_sel, reg_X_test_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. What is the classification accuracy on training and test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Without GirdSearhCV\")\n",
    "report_accuracy(\"reg_rfe\", reg_X_test_sel)\n",
    "\n",
    "print(\"With GirdSearhCV\")\n",
    "report_accuracy(\"reg_cv_rfe\", reg_X_test_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Which of the regression models appears to be better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use comparison statistics.\n",
    "print(\"Comparing all regression models f score, regression with RFE and GridsearchCV produces the best accuracy and f-score.\")\n",
    "\n",
    "# Is there any difference between two models (i.e one with selected variables and another with all variables)? \n",
    "print(\"Different accuracies, different number of variables, and different learned parameters.\")\n",
    "\n",
    "# Explain why those changes may have happened.\n",
    "print(\"GridSearchCV determines the optimal hyperparameters which could influence the learned parameters. RFE eliminates most variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 From the better model, can you identify which students to target? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you provide some descriptive summary of those students?\n",
    "print(\"Good G1 and G2 score with low number of failures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominate RFE as best model\n",
    "accuracy_overview[\"regression\"] = format_accuracy_overview(reg_Y_test, models[\"reg_cv_rfe\"].predict(reg_X_test_sel), rfe_sorted_features[:5])\n",
    "\n",
    "print(accuracy_overview[\"regression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict({\"col1\": [1, 2, 3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4. Predictive Modeling Using Neural Networks\n",
    "\n",
    "(5.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build a Neural Network model using the default setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_nn, ytr_nn = reg_X_train_array, reg_Y_train_array\n",
    "Xte_nn, yte_nn = reg_X_test_array, reg_Y_test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtr_nn, ytr_nn = X_training_decision_tree_format, Y_training_decision_tree_format\n",
    "# Xte_nn, yte_nn = X_test_decision_tree_format, Y_test_decision_tree_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = \"G3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp.fit(Xtr_nn, ytr_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. What is the network architecture of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the default neural network has {len(mlp.hidden_layer_sizes)} hidden layers(s) of size {mlp.hidden_layer_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. How many iterations are needed to train this network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the default neural network trained for {len(mlp.loss_curve_)} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Do you see any sign of over-fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.score(Xte_nn, yte_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mlp.loss_curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Did the training process converge and result in the best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.best_loss_, mlp.loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. What is the classification accuracy on the training and test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The classification on the training set compared to the test set is {mlp.score(Xtr_nn, ytr_nn)}/{mlp.score(Xte_nn, yte_nn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Refine this network by refining is with GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section a neural network's hyperparameters are tuned using GridSearchCV.\n",
    "\n",
    "* `hidden_layer_size` was optimized to ensure that that a the complexity of the model was appropriate for the problem\n",
    "* `activation` was optimized to ensure that the activation function of the model is capaable of representing the problm. Relu is max(0, x) and allows only relevant signals to pass through to the next layer. tanh is good when the sign of the signal is relevant but the magnitude needs to be limited. logistic is a mix of the two.\n",
    "\n",
    "source: https://www.quora.com/How-should-I-choose-a-proper-activation-function-for-the-neural-network?share=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"hidden_layer_sizes\": [(50), (100), (100, 100)],\n",
    "    \"activation\": ['logistic', 'tanh', 'relu'],\n",
    "}\n",
    "\n",
    "gs_mlp = GridSearchCV(MLPClassifier(max_iter=1000), params, n_jobs=-1).fit(Xtr_nn, ytr_nn)\n",
    "plt.plot(mlp.loss_curve_)\n",
    "display(\n",
    "    f\"train acc:{gs_mlp.score(Xtr_nn, ytr_nn)} test acc: {gs_mlp.score(Xte_nn, yte_nn)}\",\n",
    "    f\"best loss: {gs_mlp.best_loss_} loss:{gs_mlp.loss_}\",\n",
    "    gs_mlp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build  another Neural Network with inputs selected from RFE with regression.\n",
    "\n",
    "(Use the best model generated in Task 3) and selection with decision tree (use the best model from Task 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_nn_rfe, Xte_nn_rfe = reg_X_train_sel = rfe.transform(reg_X_train_array), rfe.transform(reg_X_test_array)\n",
    "ytr_nn_rfe, yte_nn_rfe = ytr_nn, yte_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Did feature selection help here? Any changes in network architecture? What inputs are being used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_rfe = MLPClassifier(max_iter=1000).fit(Xtr_nn_rfe, ytr_nn_rfe)\n",
    "\n",
    "plt.plot(mlp_rfe.loss_curve_)\n",
    "display(\n",
    "    f\"train acc:{mlp_rfe.score(Xtr_nn_rfe, ytr_nn_rfe)} test acc: {mlp_rfe.score(Xte_nn_rfe, yte_nn_rfe)}\",\n",
    "    f\"best loss: {mlp_rfe.best_loss_} loss:{mlp_rfe.loss_}\",\n",
    "    mlp_rfe\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What is the classification accuracy on the train and test datasets? Any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"train acc:{mlp_rfe.score(Xtr_nn_rfe, ytr_nn_rfe)} test acc: {mlp_rfe.score(Xte_nn_rfe, yte_nn_rfe)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. How many iteration are needed to train this network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was run for 100 iterations and converged at approximately 500 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Do you see any sign of over-fitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the difference between the train and test performance for the GridSearch/RFE model only approximately 4%, this is insignificant and is much lower than the 16% difference found in the pure GridSearch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Did the training process converge and result in the best model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the training process converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f. Use GridSearchCV to tune the network to see whether the change in network architecture can further improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"hidden_layer_sizes\": [(50), (100), (100, 100)],\n",
    "    \"activation\": ['logistic', 'tanh', 'relu'],\n",
    "}\n",
    "\n",
    "gs_mlp_rfe = GridSearchCV(MLPClassifier(max_iter=1000), params, n_jobs=-1).fit(Xtr_nn_rfe, ytr_nn_rfe)\n",
    "\n",
    "plt.plot(gs_mlp_rfe.best_estimator_.loss_curve_)\n",
    "display(\n",
    "    f\"train acc:{gs_mlp_rfe.score(Xtr_nn_rfe, ytr_nn_rfe)} test acc: {gs_mlp_rfe.score(Xte_nn_rfe, yte_nn_rfe)}\",\n",
    "    f\"best loss: {gs_mlp_rfe.best_estimator_.best_loss_} loss:{gs_mlp_rfe.best_estimator_.loss_}\",\n",
    "    gs_mlp_rfe\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the comparison methods, Which appears to be better?\n",
    "\n",
    "From the better model, can you identify which customers to target? Can you provide some descriptive summary of those customers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5. Comparing Predictive Models\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using the comparison methods to compare the best decsision tree model, the best regression model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Discuss the findings led by (a) ROC Chart and Index; (b) Accuracy Score; (c) Classification Report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Do all the mdoels agree on the customers' characteristics? How do they vary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Finally, based on all models and analysis, is there a particular model you will use in decision making?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Can you summarise positives and negaitives of each modelling method based on this analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree\n",
    "**Pros:**\n",
    " - Very easy to interpret. The model can be summarized into 2 simple if then rules. As can be seen in the image at 2.2.d.\n",
    " - Is very robust. It can handle most data types. Only catagorical data has to be encoded.\n",
    " - Relatively fast training time.\n",
    "\n",
    "**Cons:**\n",
    " - Cannot handel complicated relationship. This is the reason why the accuracy is not higher than 86% on the test data.\n",
    " - Is very sensitive to overfitting. As can be seen in the default decision tree that had a 100% accuracy on the training set while it only had a 82% accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "**Pros:**\n",
    " - List item\n",
    " - \n",
    "\n",
    "**Cons:**\n",
    " - List item\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "**Pros:**\n",
    " - List item\n",
    " - \n",
    "\n",
    "**Cons:**\n",
    " - List item\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CAB330]",
   "language": "python",
   "name": "conda-env-CAB330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
